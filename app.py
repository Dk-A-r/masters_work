# –ò–º–ø–æ—Ä—Ç
import re
import os
import spacy
import pandas as pd
import numpy as np
import torch
from nltk.tokenize import word_tokenize, sent_tokenize
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from natasha import Segmenter, NewsNERTagger, NewsEmbedding, Doc
import emoji
import re
import pymorphy2
import json
from lime.lime_text import LimeTextExplainer
import joblib


# –ü–æ–ª—É—á–∞–µ–º –ø—É—Ç—å –∫ —Ç–µ–∫—É—â–µ–º—É —Ñ–∞–π–ª—É
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
MODELS_DIR = os.path.join(SCRIPT_DIR, "models")

# –ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –ø—Ä–∏–∑–Ω–∞–∫–∞–º
with open(os.path.join(MODELS_DIR,"df_stats.json"), 'r', encoding='utf-8') as f:
    df_stats = json.load(f)

# –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–ø-–≤–∏—Ä—É—Å–Ω—ã—Ö —Å–ª–æ–≤
top_viral_words = joblib.load(os.path.join(MODELS_DIR, "top_viral_words.pkl"))

# –ó–∞–≥—Ä—É–∂–∞–µ–º —Ä—É—Å—Å–∫—É—é –º–æ–¥–µ–ª—å spaCy
nlp = spacy.load("ru_core_news_sm")

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π
scaler = joblib.load(os.path.join(MODELS_DIR, "scaler.pkl"))
model = joblib.load(os.path.join(MODELS_DIR, "best_model.pkl"))
bert_model = SentenceTransformer(os.path.join(MODELS_DIR, "best_sentence_transformer"))

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Natasha
segmenter = Segmenter()
emb = NewsEmbedding()
ner_tagger = NewsNERTagger(emb)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
morph = pymorphy2.MorphAnalyzer()

# –ó–∞–≥—Ä—É–∑–∫–∞ —Å–ª–µ–Ω–≥–∞
slang_dict = {}
try:
    with open(os.path.join(MODELS_DIR, "slang_dict.txt"), "r", encoding="utf-8") as f:
        for line in f:
            if "=" in line:
                k, v = line.strip().split("=", 1)
                slang_dict[k] = v
except FileNotFoundError:
    print("‚ö†Ô∏è –°–ª–æ–≤–∞—Ä—å —Å–ª–µ–Ω–≥–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω.")

# –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –¥–ª—è —Å–µ–Ω—Ç–∏–º–µ–Ω—Ç–∞
tokenizer_sentiment = AutoTokenizer.from_pretrained("cointegrated/rubert-tiny-sentiment-balanced")
model_sentiment = AutoModelForSequenceClassification.from_pretrained("cointegrated/rubert-tiny-sentiment-balanced")
# %%
# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
explainer = LimeTextExplainer(class_names=["–ù–µ –≤–∏—Ä—É—Å–Ω—ã–π", "–í–∏—Ä—É—Å–Ω—ã–π"])

def remove_repeated_letters(text, max_repeats=2):
    def reduce_repeats(match):
        char = match.group(0)[0]
        return char * max_repeats
    return re.sub(r'(.)\1{2,}', reduce_repeats, text)

def convert_emoji_to_text(text):
    return emoji.demojize(text, delimiters=(" :", ": "))

def replace_slang(text, slang_dict=None):
    words = text.split()
    replaced = [slang_dict[word] if slang_dict and word in slang_dict else word for word in words]
    return ' '.join(replaced)

def preprocess_tweet(text, slang_dict=None):
    text = str(text).lower()
    text = convert_emoji_to_text(text)
    if slang_dict:
        text = replace_slang(text, slang_dict)
    text = remove_repeated_letters(text)
    return text.strip()

def extract_features_ru(text):
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö
    if not isinstance(text, str) or len(str(text).strip()) == 0:
        print("‚ùå –û—à–∏–±–∫–∞: –¢–µ–∫—Å—Ç –ø—É—Å—Ç–æ–π –∏–ª–∏ –∏–º–µ–µ—Ç –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π —Ç–∏–ø:", repr(text))
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –¥–µ—Ñ–æ–ª—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        return {
            'tweet_length': 0,
            'word_count': 0,
            'has_hashtag': 0,
            'has_mention': 0,
            'has_url': 0,
            'contains_caps': 0,
            'contains_emoji': 0,
            'contains_question': 0,
            'contains_exclamation': 0,
            'avg_word_length': 0,
            'punctuation_density': 0,
            'sentiment_score': 0.0,
            'reading_ease': 100.0,
            'subjectivity_score': 0.0,
            'polarity_score': 0.0,
            'named_entities_count': 0,
            'keyword_density': 0,
            'contains_keywords': 0,
            'url_count': 0,
            'caps_word_count': 0,
            'emoji_count': 0,
            'sentence_count': 0
        }

    text = str(text)
    features = {}
    try:
        words = word_tokenize(text.lower())
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏: {e}")
        return {k: 0 for k in features}  # –ò–ª–∏ –≤–µ—Ä–Ω—É—Ç—å –¥–µ—Ñ–æ–ª—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è

    char_count = len(text)
    features['tweet_length'] = char_count
    word_count = len(words)
    features['word_count'] = word_count
    features['has_hashtag'] = int(bool(re.search(r'#\w+', text)))
    features['has_mention'] = int(bool(re.search(r'@\w+', text)))
    features['has_url'] = int(bool(re.search(r'https?://\S+', text)))
    caps_words = [w for w in words if w.isupper() and len(w) > 1]
    features['contains_caps'] = int(len(caps_words) > 0)
    emoji_list = [c for c in text if c in emoji.EMOJI_DATA]
    features['contains_emoji'] = int(len(emoji_list) > 0)
    features['contains_question'] = int('?' in text)
    features['contains_exclamation'] = int('!' in text)
    features['avg_word_length'] = round(sum(len(word) for word in words) / word_count, 2) if word_count else 0
    punctuation_count = sum(1 for c in text if c in '.,!?;:')
    features['punctuation_density'] = round(punctuation_count / word_count, 2) if word_count else 0
    try:
        inputs = tokenizer_sentiment(text, return_tensors="pt", truncation=True, padding=True)
        outputs = model_sentiment(**inputs)
        probs = torch.nn.functional.softmax(outputs.logits, dim=1).detach().numpy()[0]
        sentiment_score = probs[1] - probs[0]
        features['sentiment_score'] = float(sentiment_score)
        avg_word_len = sum(len(w) for w in words) / word_count if word_count else 0
        features['reading_ease'] = 100 - (avg_word_len * 10)
        features['subjectivity_score'] = 1.0 - float(abs(sentiment_score))
        features['polarity_score'] = float(probs[1])
    except Exception as e:
        features['sentiment_score'] = 0.0
        avg_word_len = sum(len(w) for w in words) / word_count if word_count else 0
        features['reading_ease'] = 100 - (avg_word_len * 10)
        features['subjectivity_score'] = 0.0
        features['polarity_score'] = 0.0
    try:
        doc = Doc(text)
        doc.segment(segmenter)
        doc.tag_ner(ner_tagger)
        entities = [(span.text, span.type) for span in doc.spans]
        features['named_entities_count'] = len(entities)
    except:
        features['named_entities_count'] = 0
    keywords_found = [k for k in KEYWORDS if k in text.lower()]
    features['keyword_density'] = round(len(keywords_found) / word_count, 2) if word_count else 0
    features['contains_keywords'] = int(len(keywords_found) > 0)
    features['url_count'] = len(re.findall(r'https?://\S+', text))
    features['caps_word_count'] = len(caps_words)
    features['emoji_count'] = len(emoji_list)
    sentences = sent_tokenize(text)
    features['sentence_count'] = len(sentences)
    return features


def is_valid_viral_word(word):
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å–ª–æ–≤–æ –ø–æ–¥—Ö–æ–¥—è—â–∏–º –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
        –≤ –∫–∞—á–µ—Å—Ç–≤–µ –≤–∏—Ä—É—Å–Ω–æ–≥–æ —Å–ª–æ–≤–∞ (–Ω–µ –∏–º—è —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–µ, –Ω–µ –¥–∞—Ç–∞ –∏ —Ç.–¥.)
        """
        # –ò—Å–∫–ª—é—á–∞–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã –≤—Ä–æ–¥–µ club12345|name, id12345, —Å—Å—ã–ª–∫–∏
        invalid_patterns = [
            r"^club\d+\|.*$",     # club12345|name
            r"^id\d+$",           # id123456
            r"^https?://.*$",     # —Å—Å—ã–ª–∫–∏
            r".*\|.*",            # –ª—é–±—ã–µ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ —Å |
        ]
        for pattern in invalid_patterns:
            if re.match(pattern, word):
                return False
    
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á–µ—Ä–µ–∑ NER: –µ—Å–ª–∏ —Å–ª–æ–≤–æ ‚Äî –∏–º–µ–Ω–æ–≤–∞–Ω–Ω–∞—è —Å—É—â–Ω–æ—Å—Ç—å, –∏—Å–∫–ª—é—á–∞–µ–º
        doc = nlp(word)
        for ent in doc.ents:
            if ent.label_ in ["PER", "PERSON", "ORG", "GPE", "LOC", "DATE", "TIME", "CARDINAL"]:
                return False
    
        # –°–ª–æ–≤–æ –ø—Ä–æ—à–ª–æ –≤—Å–µ –ø—Ä–æ–≤–µ—Ä–∫–∏
        return True


# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
def generate_strategic_recommendations(tweet_text, df_stats, top_viral_words, explanation_list=None):
    recommendations = []
    factors = {
        'tweet_length': {'value': len(tweet_text), 'threshold': df_stats['tweet_length']['good']},
        'has_hashtag': {'value': int('#' in tweet_text), 'threshold': df_stats['has_hashtag']['good']},
        'contains_emoji': {'value': int(any(c in emoji.EMOJI_DATA for c in tweet_text)), 'threshold': df_stats['contains_emoji']['good']}
    }
    if factors['tweet_length']['value'] < factors['tweet_length']['threshold']:
        recommendations.append("–£–≤–µ–ª–∏—á—å—Ç–µ –¥–ª–∏–Ω—É –ø–æ—Å—Ç–∞ ‚Äî –∫–æ—Ä–æ—Ç–∫–∏–µ –ø–æ—Å—Ç—ã —Ä–µ–∂–µ —Å—Ç–∞–Ω–æ–≤—è—Ç—Å—è –≤–∏—Ä—É—Å–Ω—ã–º–∏.")
    if factors['has_hashtag']['value'] < factors['has_hashtag']['threshold']:
        recommendations.append("–î–æ–±–∞–≤—å—Ç–µ —Ö—ç—à—Ç–µ–≥–∏ ‚Äî –æ–Ω–∏ —É–≤–µ–ª–∏—á–∏–≤–∞—é—Ç –æ—Ö–≤–∞—Ç.")
    if factors['contains_emoji']['value'] < factors['contains_emoji']['threshold']:
        recommendations.append("–î–æ–±–∞–≤—å—Ç–µ —ç–º–æ–¥–∑–∏ ‚Äî –æ–Ω–∏ –ø—Ä–∏–≤–ª–µ–∫–∞—é—Ç –≤–Ω–∏–º–∞–Ω–∏–µ.")
    if explanation_list:
        negative_words = sorted([(w, s) for w, s in explanation_list if s < -0.05], key=lambda x: x[1])
        for word, score in negative_words:
            suggestions = [w for w in top_viral_words if w != word and is_valid_viral_word(w)][:3]
            if suggestions:
                examples = "', '".join(suggestions)
                recommendations.append(f"–ó–∞–º–µ–Ω–∏—Ç–µ —Å–ª–æ–≤–æ '{word}' –Ω–∞ –±–æ–ª–µ–µ –≤–∏—Ä—É—Å–Ω–æ–µ. –ù–∞–ø—Ä–∏–º–µ—Ä: '{examples}'")
    return recommendations or ["–ü–æ—Å—Ç –≤—ã–≥–ª—è–¥–∏—Ç —Ö–æ—Ä–æ—à–æ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º."]

def predict_viral(text):
    # –ó–∞—â–∏—Ç–∞ –æ—Ç None, —á–∏—Å–µ–ª, –ø—É—Å—Ç—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
    if not isinstance(text, str) or len(str(text).strip()) == 0:
        print("‚ùå –û—à–∏–±–∫–∞: –¢–µ–∫—Å—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –Ω–µ–ø—É—Å—Ç–æ–π —Å—Ç—Ä–æ–∫–æ–π.")
        return {
            "class": "–û—à–∏–±–∫–∞",
            "confidence": 0.0,
            "explanation_html": "<div>–û—à–∏–±–∫–∞: —Ç–µ–∫—Å—Ç –ø—É—Å—Ç–æ–π –∏–ª–∏ –∏–º–µ–µ—Ç –Ω–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç.</div>",
            "recommendations": ["–¢–µ–∫—Å—Ç –ø—É—Å—Ç–æ–π –∏–ª–∏ –∏–º–µ–µ—Ç –Ω–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç."]
        }

    text = str(text).strip()
    processed_text = preprocess_tweet(text, slang_dict)
    
    # BERT —ç–º–±–µ–¥–¥–∏–Ω–≥
    bert_embedding = bert_model.encode([processed_text], batch_size=1, show_progress_bar=False, convert_to_numpy=True)

    # –ß–∏—Å–ª–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    numeric_features = extract_features_ru(processed_text)
    numeric_df = pd.DataFrame([numeric_features])
    numeric_scaled = scaler.transform(numeric_df)

    # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ
    X_combined = np.hstack((bert_embedding, numeric_scaled))

    # –ü—Ä–æ–≥–Ω–æ–∑
    proba = model.predict_proba(X_combined)[0]
    predicted_class = model.predict(X_combined)[0]

    class_label = "–î–∞" if predicted_class == 1 else "–ù–µ—Ç"
    confidence = proba[predicted_class]

    # LIME –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ
    def transform_for_lime(texts):
        processed_texts = [preprocess_tweet(t, slang_dict) for t in texts]
        bert_emb = bert_model.encode(processed_texts, batch_size=1, convert_to_numpy=True)
        return np.hstack((bert_emb, np.tile(numeric_scaled[0], (len(texts), 1))))

    exp = explainer.explain_instance(
        text,
        transform_for_lime,
        num_features=10,
        num_samples=500
    )

    # --- –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ Jupyter ---
    print("\nüìä –í–ª–∏—è–Ω–∏–µ —Å–ª–æ–≤ –Ω–∞ –≤–∏—Ä–∞–ª—å–Ω–æ—Å—Ç—å:")
    #exp.show_in_notebook(text=True, labels=(1,), predict_proba=False)
    # –í–º–µ—Å—Ç–æ show_in_notebook
    explanation = exp.as_list(label=1)
    print("Explanation for class 1:")
    print(explanation)

    explanation_html = exp.as_html()

    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    explanation_list = exp.as_list()
    recommendations = generate_strategic_recommendations(text, df_stats, top_viral_words, explanation_list)

    return {
        "class": class_label,
        "confidence": confidence,
        "explanation_html": explanation_html,
        "recommendations": recommendations
    }

text = input("–í–≤–µ–¥–∏—Ç–µ –í–∞—à–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –∑–¥–µ—Å—å: ...")

import pandas as pd
from nltk.tokenize import word_tokenize, sent_tokenize
KEYWORDS = ['–≤—ã–∏–≥—Ä–∞–π', '–ø—Ä–∏–∑', '–±–µ—Å–ø–ª–∞—Ç–Ω–æ', '–¥–µ–Ω–µ–≥', '–∫–ª–∏–∫–Ω–∏', '–∞–∫—Ü–∏—è', '—Å–∫–∏–¥–∫–∞']
result = predict_viral(text)

print(f"–ö–ª–∞—Å—Å: {result['class']}")
print(f"–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {result['confidence']:.2%}")
print("–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:")
for r in result['recommendations']:
    print(" -", r)

# –°–æ—Ö—Ä–∞–Ω—è–µ–º LIME —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –æ–Ω –¥–æ—Å—Ç—É–ø–µ–Ω
if 'explanation_html' in result:
    with open("lime_explanation.html", "w", encoding="utf-8") as f:
        f.write(result["explanation_html"])
else:
    print("‚ö†Ô∏è –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ LIME.")